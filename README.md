# Neural Network Sampling Analysis

This project explores the impact of different sampling strategies on neural network training, focusing on a comparison between traditional Monte Carlo (MC) sampling and low-discrepancy Quasi-Monte Carlo (QMC) sequences.

The study spans five diverse scenarios designed to test generalization across varying problem types and dimensionalities:
- **Six-dimensional sum of sines** â€“ a smooth, high-dimensional function.
- **BSPDE** â€“ a backward stochastic partial differential equation.
- **Airfoil pressure prediction** â€“ a physics-informed, real-world inspired scenario.
- **Projectile motion** â€“ a classical mechanics problem with known analytical solution.

By analyzing training convergence, generalization error, and robustness across these problems, this repository provides insight into the role of sampling in neural network performance.

## Goals
- Evaluate how QMC sampling compares to standard MC for neural network training.
- Analyze performance across low- and high-dimensional, smooth and complex problem types.
- Understand whether better point distributions can lead to better generalization with fewer training points.

## Contents
- ğŸ“ `data/` â€“ Sampling point generation scripts (MC and QMC)
- ğŸ“ `models/` â€“ Neural network architectures and training loops
- ğŸ“ `scenarios/` â€“ Problem definitions and ground truth evaluations
- ğŸ“Š `results/` â€“ Evaluation metrics and visualizations
