{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports üëΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import tqdm\n",
    "from itertools import product\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "from data_classes.architecture import NeuralNetworkArchitecture\n",
    "from data_classes.enums import OptimizationMethod\n",
    "from data_classes.experiment import Experiment, SamplingMethod\n",
    "from data_classes.scenario import Scenario, ScenarioSettings\n",
    "from data_classes.training_data import InputData\n",
    "from data_classes.training_config import AdamTrainingConfig, TrainingSettings, LionTrainingConfig\n",
    "from models import SequentialNeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Scenario and Training Settings üß™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "    SAMPLING_METHOD=SamplingMethod.SOBOL,\n",
    "    SCENARIO=Scenario.SUM_SINES_6D\n",
    ")\n",
    "\n",
    "# experiments = [\n",
    "#     Experiment(\n",
    "#         SAMPLING_METHOD=SamplingMethod.SOBOL,\n",
    "#         SCENARIO=Scenario.SUM_SINES\n",
    "#     ),\n",
    "#     Experiment(\n",
    "#         SAMPLING_METHOD=SamplingMethod.MC,\n",
    "#         SCENARIO=Scenario.SUM_SINES\n",
    "#     ),\n",
    "#     Experiment(\n",
    "#         SAMPLING_METHOD=SamplingMethod.SOBOL,\n",
    "#         SCENARIO=Scenario.PROJECTILE\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "scenario_settings = ScenarioSettings(experiment.SCENARIO)\n",
    "\n",
    "input_data = InputData(scenario_settings.DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN Architectures\n",
    "widths = [2,8,16,64]\n",
    "depths = [2,4,8]\n",
    "activation_functions = [torch.nn.Sigmoid, torch.nn.Tanh]\n",
    "\n",
    "# Optimizer settings\n",
    "# Adam\n",
    "adam_beta1s = [0.9]\n",
    "adam_beta2s = [0.999]\n",
    "adam_lr_wd_combos = [(1e-3,0), (1e-3,1e-6), (1e-3,1e-5), (3e-4,0), (3e-4,1e-6), (3e-4,1e-5)]\n",
    "adam_epsilons = [1e-8]\n",
    "\n",
    "# Lion\n",
    "lion_beta1s = [0.9]\n",
    "lion_beta2s = [0.99]\n",
    "lion_lr_wd_combos = [(3e-3,0),(3e-3, 1e-6),(1e-3,0),(1e-3,1e-6),(3e-3,1e-5),(1e-3,1e-5)]\n",
    "\n",
    "\n",
    "# Learing settings:\n",
    "training_set_sizes = [128,512,2048,8192]\n",
    "batch_sizes = [64,256,512,1024,2048]\n",
    "epochs = [50,100,250,500,750,1000,1250,1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_settings = []\n",
    "for width, depth, activation_func, train_set_size, batch_size, epoch in product(widths, depths, activation_functions, training_set_sizes, batch_sizes, epochs):\n",
    "    \n",
    "    nn_arch = NeuralNetworkArchitecture(\n",
    "        INPUT_DIM=scenario_settings.INPUT_DIM,\n",
    "        OUTPUT_DIM=scenario_settings.OUTPUT_DIM,\n",
    "        NUM_HIDDEN_LAYERS=width,\n",
    "        DEPTH=depth,\n",
    "        ACTIVATION_FUNCTION=activation_func\n",
    "    )\n",
    "\n",
    "    for beta1, beta2, lr_wd, epsilon in product(adam_beta1s, adam_beta2s, adam_lr_wd_combos, adam_epsilons):\n",
    "        learning_rate, weight_decay = lr_wd\n",
    "        \n",
    "        training_config = AdamTrainingConfig(\n",
    "            OPTIMIZER=OptimizationMethod.ADAM,\n",
    "            LEARNING_RATE=learning_rate,\n",
    "            REG_PARAM=weight_decay,\n",
    "            BETAS=(beta1, beta2),\n",
    "            EPS=epsilon,\n",
    "            NUM_EPOCHS=epoch,\n",
    "            BATCH_SIZE=batch_size\n",
    "        )\n",
    "\n",
    "        all_training_settings.append(TrainingSettings(\n",
    "            nn_architecture=nn_arch,\n",
    "            training_config=training_config,\n",
    "            training_set_size=train_set_size\n",
    "        ))\n",
    "    \n",
    "    for beta1, beta2, lr_wd in product(lion_beta1s, lion_beta2s, lion_lr_wd_combos):\n",
    "\n",
    "        training_config = LionTrainingConfig(\n",
    "            OPTIMIZER=OptimizationMethod.LION,\n",
    "            LEARNING_RATE=learning_rate,\n",
    "            REG_PARAM=weight_decay,\n",
    "            BETAS=(beta1, beta2),\n",
    "            NUM_EPOCHS=epoch,\n",
    "            BATCH_SIZE=batch_size\n",
    "        )\n",
    "\n",
    "        all_training_settings.append(TrainingSettings(\n",
    "            nn_architecture=nn_arch,\n",
    "            training_config=training_config,\n",
    "            training_set_size=train_set_size\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46080"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_training_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|‚ñè         | 884/46080 [17:28<1:15:56,  9.92it/s]  "
     ]
    }
   ],
   "source": [
    "output_dir = os.path.abspath(os.path.join('..', 'data', experiment.SCENARIO.value, 'output'))\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "csv_path = os.path.join(output_dir, 'results.csv')\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    version = df['version'].max() + 1\n",
    "    results_list = df.to_dict(orient='records')\n",
    "else:\n",
    "    df = pd.DataFrame(columns=['version','optimizer', 'learning_rate', 'weight_decay', 'beta_1', 'beta_2', 'eps', 'num_epochs', 'batch_size', 'training_set_size', 'train_error', 'test_error', 'train_time', 'n_layers', 'layer_depth', 'activation_function', 'sampling_method', 'scenario'])\n",
    "    version = 1\n",
    "    results_list = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for ts in tqdm.tqdm(all_training_settings, desc=\"Training Progress\"):\n",
    "    nn = SequentialNeuralNetwork(\n",
    "        net_arch=ts.nn_architecture\n",
    "    )\n",
    "    training_data = input_data.get_training_and_test_data(\n",
    "        sampling_method=experiment.SAMPLING_METHOD,\n",
    "        training_set_size=ts.training_set_size\n",
    "    )\n",
    "    nn.train(settings=ts.training_config, data=training_data)\n",
    "    nn.training_results['version'] = version\n",
    "    nn.training_results['scenario'] = experiment.SCENARIO.value\n",
    "    results_list.append(nn.training_results)\n",
    "    df_results = pd.DataFrame(results_list)\n",
    "\n",
    "    df_results.sort_values('test_error', inplace=True)\n",
    "    df_results.reset_index(drop=True, inplace=True)\n",
    "    df_results.to_csv(csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
